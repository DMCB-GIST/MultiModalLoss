{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification on synthetic data with two noise modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import seaborn as sns; sns.set_theme(color_codes=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  TensorDataset, DataLoader\n",
    "from multi_modal_loss import MultiModalLoss\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "torch.cuda.set_device(0) \n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0.0001, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >self.patience:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.task11= nn.Linear(2000, 500)\n",
    "        self.task21= nn.Linear(2000, 500)\n",
    "        self.task31= nn.Linear(2000, 500)\n",
    "        \n",
    "        self.task12= nn.Linear(500, 100)\n",
    "        self.task22= nn.Linear(500, 100)\n",
    "        self.task32= nn.Linear(500, 100)\n",
    "\n",
    "        self.task13= nn.Linear(100, 20)\n",
    "        self.task23= nn.Linear(100, 20)\n",
    "        self.task33= nn.Linear(100, 20)\n",
    "\n",
    "        self.task14= nn.Linear(20, 2)\n",
    "        self.task24= nn.Linear(20, 2)\n",
    "        self.task34= nn.Linear(20, 2)\n",
    "        \n",
    "        self.taskS= nn.Linear(6, 2)\n",
    "        \n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward_one(self,xg,xm,xs):\n",
    "        xg = self.task11(xg)\n",
    "        xm = self.task21(xm)\n",
    "        xs = self.task31(xs)         \n",
    "\n",
    "        xg = self.task12(xg)\n",
    "        xm = self.task22(xm)\n",
    "        xs = self.task32(xs)         \n",
    "        \n",
    "        xg = self.task13(xg)\n",
    "        xm = self.task23(xm)\n",
    "        xs = self.task33(xs)      \n",
    "        \n",
    "        xg = self.task14(xg)\n",
    "        xm = self.task24(xm)\n",
    "        xs = self.task34(xs)     \n",
    "        \n",
    "        xg = self.softmax(xg)\n",
    "        xm = self.softmax(xm)\n",
    "        xs = self.softmax(xs)\n",
    "        \n",
    "        xg_concat = torch.cat([xg,xm,xs], dim=1)\n",
    "        x_nn = self.taskS(xg_concat)\n",
    "        return xg,xm,xs, x_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = pd.read_csv('/data/sim_train_1_EHH.csv', header=0, index_col=None).values\n",
    "train_2 = pd.read_csv('/data/sim_train_2_EHH.csv', header=0, index_col=None).values\n",
    "train_3 = pd.read_csv('/data/sim_train_3_EHH.csv', header=0, index_col=None).values\n",
    "\n",
    "test_1 = pd.read_csv('/data/sim_test_1_EHH.csv', header=0, index_col=None).values\n",
    "test_2 = pd.read_csv('/data/sim_test_2_EHH.csv', header=0, index_col=None).values\n",
    "test_3 = pd.read_csv('/data/sim_test_3_EHH.csv', header=0, index_col=None).values\n",
    "\n",
    "train_y = pd.read_csv('/data/sim_train_y_EHH.csv', header=0, index_col=None).values\n",
    "test_y = pd.read_csv('/data/sim_test_y_EHH.csv', header=0, index_col=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500000], Loss: 0.7733\n",
      "Epoch [100/500000], Loss: 0.6304\n",
      "Epoch [200/500000], Loss: 0.5626\n",
      "Epoch [300/500000], Loss: 0.5358\n",
      "Epoch [400/500000], Loss: 0.5205\n",
      "Epoch [500/500000], Loss: 0.5087\n",
      "Epoch [600/500000], Loss: 0.4978\n",
      "Epoch [700/500000], Loss: 0.4875\n",
      "Epoch [800/500000], Loss: 0.4773\n",
      "Epoch [900/500000], Loss: 0.4669\n",
      "Epoch [1000/500000], Loss: 0.4558\n",
      "Epoch [1100/500000], Loss: 0.4451\n",
      "Epoch [1200/500000], Loss: 0.4347\n",
      "Epoch [1300/500000], Loss: 0.4248\n",
      "Epoch [1400/500000], Loss: 0.4150\n",
      "Epoch [1500/500000], Loss: 0.4057\n",
      "Epoch [1600/500000], Loss: 0.3977\n",
      "Epoch [1700/500000], Loss: 0.3893\n",
      "Epoch [1800/500000], Loss: 0.3819\n",
      "Epoch [1900/500000], Loss: 0.3748\n",
      "Epoch [2000/500000], Loss: 0.3677\n",
      "Epoch [2100/500000], Loss: 0.3610\n",
      "Epoch [2200/500000], Loss: 0.3548\n",
      "Epoch [2300/500000], Loss: 0.3489\n",
      "Epoch [2400/500000], Loss: 0.3432\n",
      "Epoch [2500/500000], Loss: 0.3375\n",
      "Epoch [2600/500000], Loss: 0.3319\n",
      "Epoch [2700/500000], Loss: 0.3259\n",
      "Epoch [2800/500000], Loss: 0.3207\n",
      "Epoch [2900/500000], Loss: 0.3157\n",
      "Epoch [3000/500000], Loss: 0.3109\n",
      "Epoch [3100/500000], Loss: 0.3063\n",
      "Epoch [3200/500000], Loss: 0.3018\n",
      "Epoch [3300/500000], Loss: 0.2972\n",
      "Epoch [3400/500000], Loss: 0.2930\n",
      "Epoch [3500/500000], Loss: 0.2889\n",
      "Epoch [3600/500000], Loss: 0.2847\n",
      "Epoch [3700/500000], Loss: 0.2806\n",
      "Epoch [3800/500000], Loss: 0.2764\n",
      "Epoch [3900/500000], Loss: 0.2725\n",
      "Epoch [4000/500000], Loss: 0.2688\n",
      "Epoch [4100/500000], Loss: 0.2651\n",
      "Epoch [4200/500000], Loss: 0.2616\n",
      "Epoch [4300/500000], Loss: 0.2582\n",
      "Epoch [4400/500000], Loss: 0.2549\n",
      "Epoch [4500/500000], Loss: 0.2516\n",
      "Epoch [4600/500000], Loss: 0.2485\n",
      "Epoch [4700/500000], Loss: 0.2454\n",
      "Epoch [4800/500000], Loss: 0.2422\n",
      "Epoch [4900/500000], Loss: 0.2388\n",
      "Epoch [5000/500000], Loss: 0.2359\n",
      "Epoch [5100/500000], Loss: 0.2330\n",
      "Epoch [5200/500000], Loss: 0.2302\n",
      "Epoch [5300/500000], Loss: 0.2275\n",
      "Epoch [5400/500000], Loss: 0.2249\n",
      "Epoch [5500/500000], Loss: 0.2223\n",
      "Epoch [5600/500000], Loss: 0.2198\n",
      "Epoch [5700/500000], Loss: 0.2173\n",
      "Epoch [5800/500000], Loss: 0.2149\n",
      "Epoch [5900/500000], Loss: 0.2123\n",
      "Epoch [6000/500000], Loss: 0.2099\n",
      "Epoch [6100/500000], Loss: 0.2076\n",
      "Epoch [6200/500000], Loss: 0.2053\n",
      "Epoch [6300/500000], Loss: 0.2030\n",
      "Epoch [6400/500000], Loss: 0.2007\n",
      "Epoch [6500/500000], Loss: 0.1986\n",
      "Epoch [6600/500000], Loss: 0.1966\n",
      "Epoch [6700/500000], Loss: 0.1946\n",
      "Epoch [6800/500000], Loss: 0.1927\n",
      "Epoch [6900/500000], Loss: 0.1908\n",
      "Epoch [7000/500000], Loss: 0.1888\n",
      "Epoch [7100/500000], Loss: 0.1870\n",
      "Epoch [7200/500000], Loss: 0.1852\n",
      "Epoch [7300/500000], Loss: 0.1835\n",
      "Epoch [7400/500000], Loss: 0.1817\n",
      "Epoch [7500/500000], Loss: 0.1800\n",
      "Epoch [7600/500000], Loss: 0.1784\n",
      "Epoch [7700/500000], Loss: 0.1768\n",
      "Epoch [7800/500000], Loss: 0.1752\n",
      "Epoch [7900/500000], Loss: 0.1736\n",
      "Epoch [8000/500000], Loss: 0.1721\n",
      "Epoch [8100/500000], Loss: 0.1706\n",
      "Epoch [8200/500000], Loss: 0.1691\n",
      "Epoch [8300/500000], Loss: 0.1674\n",
      "Epoch [8400/500000], Loss: 0.1659\n",
      "Epoch [8500/500000], Loss: 0.1645\n",
      "Epoch [8600/500000], Loss: 0.1632\n",
      "Epoch [8700/500000], Loss: 0.1618\n",
      "Epoch [8800/500000], Loss: 0.1605\n",
      "Epoch [8900/500000], Loss: 0.1592\n",
      "Epoch [9000/500000], Loss: 0.1579\n",
      "Epoch [9100/500000], Loss: 0.1566\n",
      "Epoch [9200/500000], Loss: 0.1554\n",
      "Epoch [9300/500000], Loss: 0.1542\n",
      "Epoch [9400/500000], Loss: 0.1527\n",
      "Epoch [9500/500000], Loss: 0.1516\n",
      "Epoch [9600/500000], Loss: 0.1504\n",
      "Epoch [9700/500000], Loss: 0.1492\n",
      "Epoch [9800/500000], Loss: 0.1480\n",
      "Epoch [9900/500000], Loss: 0.1469\n",
      "Epoch [10000/500000], Loss: 0.1458\n",
      "Epoch [10100/500000], Loss: 0.1447\n",
      "Epoch [10200/500000], Loss: 0.1437\n",
      "Epoch [10300/500000], Loss: 0.1426\n",
      "Epoch [10400/500000], Loss: 0.1416\n",
      "Epoch [10500/500000], Loss: 0.1406\n",
      "Epoch [10600/500000], Loss: 0.1396\n",
      "Epoch [10700/500000], Loss: 0.1386\n",
      "Epoch [10800/500000], Loss: 0.1376\n",
      "Epoch [10900/500000], Loss: 0.1367\n",
      "Epoch [11000/500000], Loss: 0.1358\n",
      "Epoch [11100/500000], Loss: 0.1347\n",
      "Epoch [11200/500000], Loss: 0.1338\n",
      "Epoch [11300/500000], Loss: 0.1328\n",
      "Epoch [11400/500000], Loss: 0.1319\n",
      "Epoch [11500/500000], Loss: 0.1310\n",
      "Epoch [11600/500000], Loss: 0.1302\n",
      "Epoch [11700/500000], Loss: 0.1293\n",
      "Epoch [11800/500000], Loss: 0.1285\n",
      "Epoch [11900/500000], Loss: 0.1277\n",
      "Epoch [12000/500000], Loss: 0.1269\n",
      "Epoch [12100/500000], Loss: 0.1261\n",
      "Epoch [12200/500000], Loss: 0.1252\n",
      "Epoch [12300/500000], Loss: 0.1244\n",
      "Epoch [12400/500000], Loss: 0.1237\n",
      "Epoch [12500/500000], Loss: 0.1229\n",
      "Epoch [12600/500000], Loss: 0.1221\n",
      "Epoch [12700/500000], Loss: 0.1214\n",
      "Epoch [12800/500000], Loss: 0.1207\n",
      "Epoch [12900/500000], Loss: 0.1199\n",
      "Epoch [13000/500000], Loss: 0.1192\n",
      "Epoch [13100/500000], Loss: 0.1185\n",
      "Epoch [13200/500000], Loss: 0.1178\n",
      "Epoch [13300/500000], Loss: 0.1171\n",
      "Epoch [13400/500000], Loss: 0.1164\n",
      "Epoch [13500/500000], Loss: 0.1158\n",
      "Epoch [13600/500000], Loss: 0.1151\n",
      "Epoch [13700/500000], Loss: 0.1144\n",
      "Epoch [13800/500000], Loss: 0.1138\n",
      "Epoch [13900/500000], Loss: 0.1131\n",
      "Epoch [14000/500000], Loss: 0.1124\n",
      "Epoch [14100/500000], Loss: 0.1118\n",
      "Epoch [14200/500000], Loss: 0.1111\n",
      "Epoch [14300/500000], Loss: 0.1105\n",
      "Epoch [14400/500000], Loss: 0.1099\n",
      "Epoch [14500/500000], Loss: 0.1092\n",
      "Epoch [14600/500000], Loss: 0.1086\n",
      "Epoch [14700/500000], Loss: 0.1080\n",
      "Epoch [14800/500000], Loss: 0.1074\n",
      "Epoch [14900/500000], Loss: 0.1063\n",
      "Epoch [15000/500000], Loss: 0.1055\n",
      "Epoch [15100/500000], Loss: 0.1049\n",
      "Epoch [15200/500000], Loss: 0.1042\n",
      "Epoch [15300/500000], Loss: 0.1036\n",
      "Epoch [15400/500000], Loss: 0.1031\n",
      "Epoch [15500/500000], Loss: 0.1025\n",
      "Epoch [15600/500000], Loss: 0.1020\n",
      "Epoch [15700/500000], Loss: 0.1014\n",
      "Epoch [15800/500000], Loss: 0.1009\n",
      "Epoch [15900/500000], Loss: 0.1003\n",
      "Epoch [16000/500000], Loss: 0.0997\n",
      "Epoch [16100/500000], Loss: 0.0991\n",
      "Epoch [16200/500000], Loss: 0.0983\n",
      "Epoch [16300/500000], Loss: 0.0977\n",
      "Epoch [16400/500000], Loss: 0.0972\n",
      "Epoch [16500/500000], Loss: 0.0966\n",
      "Epoch [16600/500000], Loss: 0.0961\n",
      "Epoch [16700/500000], Loss: 0.0956\n",
      "Epoch [16800/500000], Loss: 0.0951\n",
      "Epoch [16900/500000], Loss: 0.0946\n",
      "Epoch [17000/500000], Loss: 0.0941\n",
      "Epoch [17100/500000], Loss: 0.0936\n",
      "Epoch [17200/500000], Loss: 0.0931\n",
      "Epoch [17300/500000], Loss: 0.0926\n",
      "Epoch [17400/500000], Loss: 0.0922\n",
      "Epoch [17500/500000], Loss: 0.0917\n",
      "Epoch [17600/500000], Loss: 0.0912\n",
      "Epoch [17700/500000], Loss: 0.0902\n",
      "Epoch [17800/500000], Loss: 0.0896\n",
      "Epoch [17900/500000], Loss: 0.0891\n",
      "Epoch [18000/500000], Loss: 0.0886\n",
      "Epoch [18100/500000], Loss: 0.0878\n",
      "Epoch [18200/500000], Loss: 0.0862\n",
      "Epoch [18300/500000], Loss: 0.0853\n",
      "Epoch [18400/500000], Loss: 0.0848\n",
      "Epoch [18500/500000], Loss: 0.0842\n",
      "Epoch [18600/500000], Loss: 0.0836\n",
      "Epoch [18700/500000], Loss: 0.0831\n",
      "Epoch [18800/500000], Loss: 0.0824\n",
      "Epoch [18900/500000], Loss: 0.0820\n",
      "Epoch [19000/500000], Loss: 0.0816\n",
      "Epoch [19100/500000], Loss: 0.0812\n",
      "Epoch [19200/500000], Loss: 0.0808\n",
      "Epoch [19300/500000], Loss: 0.0804\n",
      "Epoch [19400/500000], Loss: 0.0800\n",
      "Epoch [19500/500000], Loss: 0.0797\n",
      "Epoch [19600/500000], Loss: 0.0793\n",
      "Epoch [19700/500000], Loss: 0.0789\n",
      "Epoch [19800/500000], Loss: 0.0785\n",
      "Epoch [19900/500000], Loss: 0.0782\n",
      "Epoch [20000/500000], Loss: 0.0778\n",
      "Epoch [20100/500000], Loss: 0.0775\n",
      "Epoch [20200/500000], Loss: 0.0771\n",
      "Epoch [20300/500000], Loss: 0.0768\n",
      "Epoch [20400/500000], Loss: 0.0764\n",
      "Epoch [20500/500000], Loss: 0.0761\n",
      "Epoch [20600/500000], Loss: 0.0757\n",
      "Epoch [20700/500000], Loss: 0.0754\n",
      "Epoch [20800/500000], Loss: 0.0750\n",
      "Epoch [20900/500000], Loss: 0.0747\n",
      "Epoch [21000/500000], Loss: 0.0744\n",
      "Epoch [21100/500000], Loss: 0.0741\n",
      "Epoch [21200/500000], Loss: 0.0737\n",
      "Epoch [21300/500000], Loss: 0.0734\n",
      "Epoch [21400/500000], Loss: 0.0731\n",
      "Epoch [21500/500000], Loss: 0.0728\n",
      "Epoch [21600/500000], Loss: 0.0725\n",
      "Epoch [21700/500000], Loss: 0.0722\n",
      "Epoch [21800/500000], Loss: 0.0719\n",
      "Epoch [21900/500000], Loss: 0.0716\n",
      "Epoch [22000/500000], Loss: 0.0713\n",
      "Epoch [22100/500000], Loss: 0.0710\n",
      "Epoch [22200/500000], Loss: 0.0705\n",
      "Epoch [22300/500000], Loss: 0.0701\n",
      "Epoch [22400/500000], Loss: 0.0698\n",
      "Epoch [22500/500000], Loss: 0.0695\n",
      "Epoch [22600/500000], Loss: 0.0693\n",
      "Epoch [22700/500000], Loss: 0.0690\n",
      "Epoch [22800/500000], Loss: 0.0687\n",
      "Epoch [22900/500000], Loss: 0.0685\n",
      "Epoch [23000/500000], Loss: 0.0682\n",
      "Epoch [23100/500000], Loss: 0.0679\n",
      "Epoch [23200/500000], Loss: 0.0677\n",
      "Epoch [23300/500000], Loss: 0.0673\n",
      "Epoch [23400/500000], Loss: 0.0671\n",
      "Epoch [23500/500000], Loss: 0.0668\n",
      "Epoch [23600/500000], Loss: 0.0666\n",
      "Epoch [23700/500000], Loss: 0.0663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23800/500000], Loss: 0.0661\n",
      "Epoch [23900/500000], Loss: 0.0659\n",
      "Epoch [24000/500000], Loss: 0.0656\n",
      "Epoch [24100/500000], Loss: 0.0653\n",
      "Epoch [24200/500000], Loss: 0.0650\n",
      "Epoch [24300/500000], Loss: 0.0648\n",
      "Epoch [24400/500000], Loss: 0.0646\n",
      "Epoch [24500/500000], Loss: 0.0644\n",
      "Epoch [24600/500000], Loss: 0.0642\n",
      "Epoch [24700/500000], Loss: 0.0639\n",
      "Epoch [24800/500000], Loss: 0.0637\n",
      "Epoch [24900/500000], Loss: 0.0675\n",
      "Epoch [25000/500000], Loss: 0.0628\n",
      "Epoch [25100/500000], Loss: 0.0624\n",
      "Epoch [25200/500000], Loss: 0.0621\n",
      "Epoch [25300/500000], Loss: 0.0619\n",
      "Epoch [25400/500000], Loss: 0.0616\n",
      "Epoch [25500/500000], Loss: 0.0613\n",
      "Epoch [25600/500000], Loss: 0.0611\n",
      "Epoch [25700/500000], Loss: 0.0608\n",
      "Epoch [25800/500000], Loss: 0.0606\n",
      "Epoch [25900/500000], Loss: 0.0604\n",
      "Epoch [26000/500000], Loss: 0.0602\n",
      "Epoch [26100/500000], Loss: 0.0600\n",
      "Epoch [26200/500000], Loss: 0.0598\n",
      "Epoch [26300/500000], Loss: 0.0596\n",
      "Epoch [26400/500000], Loss: 0.0594\n",
      "Epoch [26500/500000], Loss: 0.0592\n",
      "Epoch [26600/500000], Loss: 0.0590\n",
      "Epoch [26700/500000], Loss: 0.0585\n",
      "Epoch [26800/500000], Loss: 0.0584\n",
      "Epoch [26900/500000], Loss: 0.0582\n",
      "Epoch [27000/500000], Loss: 0.0580\n",
      "Epoch [27100/500000], Loss: 0.0579\n",
      "Epoch [27200/500000], Loss: 0.0577\n",
      "Epoch [27300/500000], Loss: 0.0575\n",
      "Epoch [27400/500000], Loss: 0.0574\n",
      "Epoch [27500/500000], Loss: 0.0571\n",
      "Epoch [27600/500000], Loss: 0.0569\n",
      "Epoch [27700/500000], Loss: 0.0568\n",
      "Epoch [27800/500000], Loss: 0.0566\n",
      "Epoch [27900/500000], Loss: 0.0557\n",
      "Epoch [28000/500000], Loss: 0.0553\n",
      "Epoch [28100/500000], Loss: 0.0551\n",
      "Epoch [28200/500000], Loss: 0.0549\n",
      "Epoch [28300/500000], Loss: 0.0547\n",
      "Epoch [28400/500000], Loss: 0.0546\n",
      "Epoch [28500/500000], Loss: 0.0544\n",
      "Epoch [28600/500000], Loss: 0.0543\n",
      "Epoch [28700/500000], Loss: 0.0541\n",
      "Epoch [28800/500000], Loss: 0.0539\n",
      "Epoch [28900/500000], Loss: 0.0538\n",
      "Epoch [29000/500000], Loss: 0.0536\n",
      "Epoch [29100/500000], Loss: 0.0535\n",
      "Epoch [29200/500000], Loss: 0.0534\n",
      "Epoch [29300/500000], Loss: 0.0532\n",
      "Epoch [29400/500000], Loss: 0.0531\n",
      "Epoch [29500/500000], Loss: 0.0530\n",
      "Epoch [29600/500000], Loss: 0.0528\n",
      "Epoch [29700/500000], Loss: 0.0527\n",
      "Epoch [29800/500000], Loss: 0.0526\n",
      "Epoch [29900/500000], Loss: 0.0525\n",
      "Epoch [30000/500000], Loss: 0.0524\n",
      "Epoch [30100/500000], Loss: 0.0522\n",
      "Epoch [30200/500000], Loss: 0.0521\n",
      "Epoch [30300/500000], Loss: 0.0520\n",
      "Epoch [30400/500000], Loss: 0.0519\n",
      "Epoch [30500/500000], Loss: 0.0518\n",
      "Epoch [30600/500000], Loss: 0.0516\n",
      "Epoch [30700/500000], Loss: 0.0515\n",
      "Epoch [30800/500000], Loss: 0.0514\n",
      "Epoch [30900/500000], Loss: 0.0513\n",
      "Epoch [31000/500000], Loss: 0.0512\n",
      "Epoch [31100/500000], Loss: 0.0511\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "earlyStoppingPatience = 100\n",
    "learningRate= 0.00005\n",
    "weightDecay =0.0001\n",
    "\n",
    "y_train = train_y.flatten().astype(int)\n",
    "y_test = test_y.flatten().astype(int)\n",
    "\n",
    "Xg = torch.tensor(train_1, dtype=torch.float32).cuda()\n",
    "Xm = torch.tensor(train_2, dtype=torch.float32).cuda()\n",
    "Xs = torch.tensor(train_3, dtype=torch.float32).cuda()\n",
    "\n",
    "Xg_test = torch.tensor(test_1, dtype=torch.float32).cuda()\n",
    "Xm_test = torch.tensor(test_2, dtype=torch.float32).cuda()\n",
    "Xs_test = torch.tensor(test_3, dtype=torch.float32).cuda()\n",
    "\n",
    "y = torch.LongTensor(y_train).cuda()\n",
    "\n",
    "ds = TensorDataset(Xg, Xm,Xs,y)\n",
    "loader  = DataLoader(ds, batch_size=y_train.shape[0],shuffle=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "num_epochs = 500000 \n",
    "net = DNN()\n",
    "net = net.to(device)\n",
    "early_stopping = EarlyStopping(patience=earlyStoppingPatience, verbose=False)\n",
    "CELoss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam([{\"params\": net.parameters(), \"lr\": learningRate,  \"weight_decay\":weightDecay}])\n",
    "\n",
    "for epoch in (range(num_epochs)):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        xg,xm,xs, y = data\n",
    "        _,_,_, output = net.forward_one(xg,xm,xs)\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = CELoss(output, y.view(-1))\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    early_stopping(running_loss, net)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    if (epoch+1) % 100 == 0 or epoch == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1,  num_epochs, running_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN  ACC 0.79440\n"
     ]
    }
   ],
   "source": [
    "test1,test2,test3,output = net.forward_one(Xg_test.clone().detach(),Xm_test.clone().detach(),Xs_test.clone().detach())\n",
    "prob_test = output.cpu().detach().numpy()\n",
    "\n",
    "prob_test = softmax(prob_test, axis=1)\n",
    "prob_test =prob_test[:,1] \n",
    "print (\"NN  ACC %.5f\" %(accuracy_score(list(y_test),np.where(prob_test > 0.5, 1, 0) ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histofram of the predicted classification probability conditioned on true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Proportion'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "plt.figure(figsize=(5,3))\n",
    "\n",
    "data = pd.concat([pd.DataFrame(np.stack([test1.cpu().detach().numpy()[:5000,0].T,test2.cpu().detach().numpy()[:5000,0].T,test3.cpu().detach().numpy()[:5000,0].T])).T,pd.DataFrame(np.stack([test1.cpu().detach().numpy()[5000:,1].T,test2.cpu().detach().numpy()[5000:,1].T,test3.cpu().detach().numpy()[5000:,1].T])).T])\n",
    "sns.histplot(data=data,bins=10, palette=['#029e9e', '#d9601a','#db1a5e'], stat=\"proportion\", alpha  = 0.65, edgecolor='k', linewidth=2,common_norm = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
